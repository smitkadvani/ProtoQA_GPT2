{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm as notebook_tq\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/home/skadvani/ProtoQA_GPT2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ProtoQA_GPT2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-589cb2cf8c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ProtoQA_GPT2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ProtoQA_GPT2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('ProtoQA_GPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd  \n",
    "\n",
    "# test_file_path = 'ProtoQA_GPT2/data/dev/dev.crowdsourced.jsonl'\n",
    "# predicted_path = 'ProtoQA_GPT2/sample_answer.json'\n",
    "correct_label_file_path =  'data/train/train.jsonl'\n",
    "predicted_path =  'final_outputsample_answers.json'\n",
    "predicted_df = pd.read_json(predicted_path,lines=True)\n",
    "\n",
    "correct_label_df = pd.read_json(correct_label_file_path,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import string\n",
    "\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function checks similarity of word1, word2 have multiple words in it \n",
    "e.g. \n",
    "'''\n",
    "import re\n",
    "def removePunctuation(word):\n",
    "    return re.sub(r\"[,.;@#?!/&$]+\\ *\", \" \", word)\n",
    "def checkPairwise(word1,word2):\n",
    "    temp = 0.0\n",
    "    word1, word2= removePunctuation(word1), removePunctuation(word2)\n",
    "    words1 = word1.split(\" \")\n",
    "    words2 = word2.split(\" \")\n",
    "    #print(words1, \" fd \",  words2)\n",
    "    try:\n",
    "        for word1 in words1:\n",
    "            sim = []\n",
    "            for word2 in words2:\n",
    "                temp += wv.similarity(word1,word2)\n",
    "    except:\n",
    "        pass\n",
    "    if ((len(word1) + len(word2)) == 0):\n",
    "        return 0\n",
    "    return temp / (len(word1) + len(word2))\n",
    "\n",
    "#Function checks gensim similarity between single words and list of correct keys \n",
    "# e.g. : (bottle, ['key', 'chain', 'water bottle'] like that   \n",
    "def checkSimilarity(candidateList, key):\n",
    "    maxx = 0\n",
    "    for word in candidateList:\n",
    "        maxx = max(checkPairwise(word,key), maxx)\n",
    "    if maxx > 0.2:return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Token:  30157\n",
      "False Token:  109907\n",
      "Total : 140064\n"
     ]
    }
   ],
   "source": [
    "correct_token = 0\n",
    "false_token = 0\n",
    "label = 0\n",
    "generate_dataset=[]\n",
    "text_data=[]\n",
    "label_data=[]\n",
    "correct = open('correct.txt','w')\n",
    "incorrect = open('incorrect.txt','w')\n",
    "for index in range(8782):\n",
    "    question_id = correct_label_df['metadata'][index]['id']\n",
    "    question = correct_label_df['question'][index]['original']\n",
    "    for token in list(predicted_df[question_id][0].keys()):\n",
    "        if checkSimilarity(list((correct_label_df['answers'][index]['raw']).keys()),token):\n",
    "        #if token in list((correct_label_df['answers'][index]['raw']).keys()):\n",
    "            label = 0\n",
    "            correct.write((\" \".join(list((correct_label_df['answers'][index]['raw']).keys())) + \" \" +token))\n",
    "            correct_token+=1\n",
    "        else:\n",
    "            label = 1\n",
    "            #print(\"'\"+token+\"'\", (correct_label_df['answers'][index]['raw']).keys())\n",
    "            incorrect.write(\" \".join(list((correct_label_df['answers'][index]['raw']).keys()))+ \" \" +token)\n",
    "            false_token+=1\n",
    "        temp={}\n",
    "        label_data.append(str(label))\n",
    "        temp['label'] = label\n",
    "        temp['text'] = question + \" \" + token\n",
    "        text_data.append(question + \" \" + token)\n",
    "        \n",
    "        generate_dataset.append(temp)\n",
    "print('Correct Token: ', correct_token)\n",
    "print('False Token: ', false_token)\n",
    "total_token = correct_token + false_token\n",
    "print('Total : '+str(total_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, max_length = 20, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939252"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = int(total_token*0.80)\n",
    "# import io\n",
    "# train_file = open('train_file_data',mode='w', encoding='utf-8')\n",
    "# test_file = open('test_file_data','w')\n",
    "# train_text_data = text_data[:training_size]\n",
    "# test_text_data = text_data[training_size:]\n",
    "\n",
    "# train_file.write('\\n'.join(train_text_data))\n",
    "# test_file.write('\\n'.join(test_text_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text_list = []\n",
    "train_label_list = []\n",
    "test_text_list = []\n",
    "test_label_list = []\n",
    "for index in range(0,training_size):\n",
    "    train_text_list.append(generate_dataset[index]['text'])\n",
    "    train_label_list.append(generate_dataset[index]['label'])\n",
    "for index in range(training_size,len(generate_dataset)):\n",
    "    test_text_list.append(generate_dataset[index]['text'])\n",
    "    test_label_list.append(generate_dataset[index]['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict, Dataset\n",
    "# train_df = pd.DataFrame({\n",
    "#      \"text\" : train_text_list,\n",
    "#      \"label\" : train_label_list\n",
    "# })\n",
    "\n",
    "# test_df = pd.DataFrame({\n",
    "#      \"text\" : test_text_list,\n",
    "#      \"label\" : test_label_list\n",
    "# })\n",
    "# train_dataset = Dataset.from_dict(train_df)\n",
    "# test_dataset = Dataset.from_dict(test_df)\n",
    "# my_dataset_dict = DatasetDict({\"train\":train_dataset,\"test\":test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4110bd76d833476a8c9a817c516befdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc74347a3e049708245053083af7f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenized_my_dataset_dict = my_dataset_dict.map(preprocess_function, batched=True)\n",
    "# # tokenized_my_dataset_dict = tokenized_my_dataset_dict.remove_columns(\n",
    "# #     my_dataset_dict[\"train\"].column_names\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# def read_imdb_split(split_dir):\n",
    "#     split_dir = Path(split_dir)\n",
    "#     texts = []\n",
    "#     labels = []\n",
    "#     for label_dir in [\"pos\", \"neg\"]:\n",
    "#         for text_file in (split_dir/label_dir).iterdir():\n",
    "#             texts.append(text_file.read_text())\n",
    "#             labels.append(0 if label_dir is \"neg\" else 1)\n",
    "#     return texts, labels\n",
    "\n",
    "# train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "# test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_text_list, train_label_list, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text_list, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ProtoQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ProtoQADataset(train_encodings, train_labels)\n",
    "val_dataset = ProtoQADataset(val_encodings, val_labels)\n",
    "test_dataset = ProtoQADataset(test_encodings, test_label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 28013\n",
      "  Batch size = 64\n",
      "/nas/home/skadvani/anaconda3/envs/protoqa/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.105601   1.8828943]\n",
      " [-2.2609258  2.0012233]\n",
      " [-1.8072803  1.5793715]\n",
      " ...\n",
      " [-1.192181   1.0203533]\n",
      " [-1.8413154  1.5331643]\n",
      " [-1.9574525  1.5936884]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test_preds = trainer.predict(test_dataset).predictions\n",
    "print(test_preds)\n",
    "print((test_preds[:,0] > 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10780708956555884\n",
      "0.10294511378848728\n"
     ]
    }
   ],
   "source": [
    "sum = 0 \n",
    "for label in test_dataset.labels:\n",
    "    if label == 0: sum+=1\n",
    "print(sum/len(test_dataset.labels))\n",
    "\n",
    "sum = 0 \n",
    "for label in train_dataset.labels:\n",
    "    if label == 0: sum+=1\n",
    "print(sum/len(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     print('epoch ', epoch)\n",
    "#     for batch in train_loader:\n",
    "#         optim.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
